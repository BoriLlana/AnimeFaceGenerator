{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch     # PyTorch\n",
    "import datasets  # Huggingface Datasets; actual data\n",
    "\n",
    "from IPython.display import display  # image display\n",
    "import matplotlib.pyplot as plt      # image display\n",
    "%matplotlib inline                   \n",
    "import random\n",
    "#Torchvision functions\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms.functional as functional\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/boranollana/.cache/huggingface/datasets/DrishtiSharma___parquet/DrishtiSharma--Anime-Face-Dataset-1886e0ceeb6ab6bb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1209e6b2d8c64588a0ff57bb7639bf10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"DrishtiSharma/Anime-Face-Dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dont run the next block, torch.(con)cat fucks up with large tensors. use tim's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all images from dictionary to list\n",
    "#all_images = dataset['train'][:63565]['image']\n",
    "#resizing the first image\n",
    "#resized_image = functional.resize(all_images[0], size=[64])\n",
    "#saving the image to a torch tensor\n",
    "#train_tensor = train_tensor = ToTensor()(resized_image).unsqueeze(0)\n",
    "#for the rest of the images, resize then concatinate to that tensor, using dimension 0.\n",
    "for i in range(27528, len(all_images)):\n",
    "    resized_image = functional.resize(all_images[i], size=[64])\n",
    "    train_tensor = torch.cat((train_tensor,ToTensor()(resized_image).unsqueeze(0)), 0)\n",
    "    #if i%10000 == 0:  #I used these as a sanity check for my loop, ignore\n",
    "    #    print('i = ', i)\n",
    "\n",
    "#Saving the tensor so we dont run the above code multiple times.\n",
    "torch.save(train_tensor, 'images.pt')\n",
    "#If everything went well, we will see a tensor of size [63565, 3, 64, 64]:\n",
    "train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making progress at  0 out of  63565\n",
      "Making progress at  1000 out of  63565\n",
      "Making progress at  2000 out of  63565\n",
      "Making progress at  3000 out of  63565\n",
      "Making progress at  4000 out of  63565\n",
      "Making progress at  5000 out of  63565\n",
      "Making progress at  6000 out of  63565\n",
      "Making progress at  7000 out of  63565\n",
      "Making progress at  8000 out of  63565\n",
      "Making progress at  9000 out of  63565\n",
      "Making progress at  10000 out of  63565\n",
      "Making progress at  11000 out of  63565\n",
      "Making progress at  12000 out of  63565\n",
      "Making progress at  13000 out of  63565\n",
      "Making progress at  14000 out of  63565\n",
      "Making progress at  15000 out of  63565\n",
      "Making progress at  16000 out of  63565\n",
      "Making progress at  17000 out of  63565\n",
      "Making progress at  18000 out of  63565\n",
      "Making progress at  19000 out of  63565\n",
      "Making progress at  20000 out of  63565\n",
      "Making progress at  21000 out of  63565\n",
      "Making progress at  22000 out of  63565\n",
      "Making progress at  23000 out of  63565\n",
      "Making progress at  24000 out of  63565\n",
      "Making progress at  25000 out of  63565\n",
      "Making progress at  26000 out of  63565\n",
      "Making progress at  27000 out of  63565\n",
      "Making progress at  28000 out of  63565\n",
      "Making progress at  29000 out of  63565\n",
      "Making progress at  30000 out of  63565\n",
      "Making progress at  31000 out of  63565\n",
      "Making progress at  32000 out of  63565\n",
      "Making progress at  33000 out of  63565\n",
      "Making progress at  34000 out of  63565\n",
      "Making progress at  35000 out of  63565\n",
      "Making progress at  36000 out of  63565\n",
      "Making progress at  37000 out of  63565\n",
      "Making progress at  38000 out of  63565\n",
      "Making progress at  39000 out of  63565\n",
      "Making progress at  40000 out of  63565\n",
      "Making progress at  41000 out of  63565\n",
      "Making progress at  42000 out of  63565\n",
      "Making progress at  43000 out of  63565\n",
      "Making progress at  44000 out of  63565\n",
      "Making progress at  45000 out of  63565\n",
      "Making progress at  46000 out of  63565\n",
      "Making progress at  47000 out of  63565\n",
      "Making progress at  48000 out of  63565\n",
      "Making progress at  49000 out of  63565\n",
      "Making progress at  50000 out of  63565\n",
      "Making progress at  51000 out of  63565\n",
      "Making progress at  52000 out of  63565\n",
      "Making progress at  53000 out of  63565\n",
      "Making progress at  54000 out of  63565\n",
      "Making progress at  55000 out of  63565\n",
      "Making progress at  56000 out of  63565\n",
      "Making progress at  57000 out of  63565\n",
      "Making progress at  58000 out of  63565\n",
      "Making progress at  59000 out of  63565\n",
      "Making progress at  60000 out of  63565\n",
      "Making progress at  61000 out of  63565\n",
      "Making progress at  62000 out of  63565\n",
      "Making progress at  63000 out of  63565\n"
     ]
    }
   ],
   "source": [
    "# Set the output size\n",
    "output_size = (64, 64)\n",
    "\n",
    "# Define a transform to resize the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(output_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create an empty list to store the resized images as tensors\n",
    "resized_images = []\n",
    "\n",
    "# Loop through each image in the dataset and resize it\n",
    "for i in range( len(dataset['train']) ):\n",
    "    image = dataset['train'][i]['image']  # Get the i-th image and its label\n",
    "    image_resized = transform(image)  # Resize the image\n",
    "    resized_images.append(image_resized)  # Add the resized image to the list\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"Making progress at \",i,\"out of \",len(dataset['train']))\n",
    "\n",
    "# Convert the list of tensors to a tensor stack\n",
    "resized_images_tensor = torch.stack(resized_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63565, 3, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving tensor\n",
    "torch.save(resized_images_tensor, 'images.pt')\n",
    "resized_images_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63565, 3, 64, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading tensor:\n",
    "data = torch.load('./images.pt')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional imports:\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some hyperparam init:\n",
    "bs = 5000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GANs we dont need a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 3, 64, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataloader from our tensor\n",
    "images = DataLoader(data, batch_size=bs, shuffle=False)\n",
    "#batch shape:\n",
    "next(iter(images)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our discriminator is basically a CNN, whereas our generator is basically the equivalent of a cnn but instead of convolutions, we have the gradients of those convolutions with respect to our inputs;\n",
    "\n",
    "Discriminator: Conv2d\n",
    "Generator: ConvTranspose2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new dependency:\n",
    "from torch import nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        nn.Sequential(\n",
    "            #3 channels to 64, \n",
    "            #kernel 3x3 so filters: 3x3x3, \n",
    "            #stride(slide) 1, \n",
    "            #no padding needed since our images are already preprocessed and centered.\n",
    "\n",
    "            #gonna add more convolutions so it has a bigger viewing window, check ipad drawing for reference\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64), #batch normalization size -> same as image size\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 1),\n",
    "            nn.Flatten(),#required output: some_sizex1x1\n",
    "            nn.Sigmoid()#either pass the discriminator or not.0/1 Tanh can work as well\n",
    "        )\n",
    "    #forward pass, calls the initalizer\n",
    "    def forward(self, input):\n",
    "        output = self.nn.Sequential(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_size = 256 #same output from discriminator: 256x1x1\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        nn.Sequential(\n",
    "            nn.ConvTranspose2d(some_size, 128, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64), #batch normalization size -> same as image size\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=1, padding=0),\n",
    "            nn.Sigmoid() #either this or tanh can work as well, maybe tune this?\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.nn.Sequential(input)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes to self: Re-check prof. alvarez notes for convtranspose2d.\n",
    "Great additional article for convtranspose2d: https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
