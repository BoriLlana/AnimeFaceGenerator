{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch     # PyTorch\n",
    "import datasets  # Huggingface Datasets; actual data\n",
    "\n",
    "from IPython.display import display  # image display\n",
    "import matplotlib.pyplot as plt      # image display\n",
    "%matplotlib inline                   \n",
    "import random\n",
    "#Torchvision functions\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms.functional as functional\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/boranollana/.cache/huggingface/datasets/DrishtiSharma___parquet/DrishtiSharma--Anime-Face-Dataset-1886e0ceeb6ab6bb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0037d13bc7b24eafb16ce9f2958ec32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"DrishtiSharma/Anime-Face-Dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dont run the next block, torch.(con)cat fucks up with large tensors. use tim's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all images from dictionary to list\n",
    "#all_images = dataset['train'][:63565]['image']\n",
    "#resizing the first image\n",
    "#resized_image = functional.resize(all_images[0], size=[64])\n",
    "#saving the image to a torch tensor\n",
    "#train_tensor = train_tensor = ToTensor()(resized_image).unsqueeze(0)\n",
    "#for the rest of the images, resize then concatinate to that tensor, using dimension 0.\n",
    "for i in range(27528, len(all_images)):\n",
    "    resized_image = functional.resize(all_images[i], size=[64])\n",
    "    train_tensor = torch.cat((train_tensor,ToTensor()(resized_image).unsqueeze(0)), 0)\n",
    "    #if i%10000 == 0:  #I used these as a sanity check for my loop, ignore\n",
    "    #    print('i = ', i)\n",
    "\n",
    "#Saving the tensor so we dont run the above code multiple times.\n",
    "torch.save(train_tensor, 'images.pt')\n",
    "#If everything went well, we will see a tensor of size [63565, 3, 64, 64]:\n",
    "train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making progress at  0 out of  63565\n",
      "Making progress at  1000 out of  63565\n",
      "Making progress at  2000 out of  63565\n",
      "Making progress at  3000 out of  63565\n",
      "Making progress at  4000 out of  63565\n",
      "Making progress at  5000 out of  63565\n",
      "Making progress at  6000 out of  63565\n",
      "Making progress at  7000 out of  63565\n",
      "Making progress at  8000 out of  63565\n",
      "Making progress at  9000 out of  63565\n",
      "Making progress at  10000 out of  63565\n",
      "Making progress at  11000 out of  63565\n",
      "Making progress at  12000 out of  63565\n",
      "Making progress at  13000 out of  63565\n",
      "Making progress at  14000 out of  63565\n",
      "Making progress at  15000 out of  63565\n",
      "Making progress at  16000 out of  63565\n",
      "Making progress at  17000 out of  63565\n",
      "Making progress at  18000 out of  63565\n",
      "Making progress at  19000 out of  63565\n",
      "Making progress at  20000 out of  63565\n",
      "Making progress at  21000 out of  63565\n",
      "Making progress at  22000 out of  63565\n",
      "Making progress at  23000 out of  63565\n",
      "Making progress at  24000 out of  63565\n",
      "Making progress at  25000 out of  63565\n",
      "Making progress at  26000 out of  63565\n",
      "Making progress at  27000 out of  63565\n",
      "Making progress at  28000 out of  63565\n",
      "Making progress at  29000 out of  63565\n",
      "Making progress at  30000 out of  63565\n",
      "Making progress at  31000 out of  63565\n",
      "Making progress at  32000 out of  63565\n",
      "Making progress at  33000 out of  63565\n",
      "Making progress at  34000 out of  63565\n",
      "Making progress at  35000 out of  63565\n",
      "Making progress at  36000 out of  63565\n",
      "Making progress at  37000 out of  63565\n",
      "Making progress at  38000 out of  63565\n",
      "Making progress at  39000 out of  63565\n",
      "Making progress at  40000 out of  63565\n",
      "Making progress at  41000 out of  63565\n",
      "Making progress at  42000 out of  63565\n",
      "Making progress at  43000 out of  63565\n",
      "Making progress at  44000 out of  63565\n",
      "Making progress at  45000 out of  63565\n",
      "Making progress at  46000 out of  63565\n",
      "Making progress at  47000 out of  63565\n",
      "Making progress at  48000 out of  63565\n",
      "Making progress at  49000 out of  63565\n",
      "Making progress at  50000 out of  63565\n",
      "Making progress at  51000 out of  63565\n",
      "Making progress at  52000 out of  63565\n",
      "Making progress at  53000 out of  63565\n",
      "Making progress at  54000 out of  63565\n",
      "Making progress at  55000 out of  63565\n",
      "Making progress at  56000 out of  63565\n",
      "Making progress at  57000 out of  63565\n",
      "Making progress at  58000 out of  63565\n",
      "Making progress at  59000 out of  63565\n",
      "Making progress at  60000 out of  63565\n",
      "Making progress at  61000 out of  63565\n",
      "Making progress at  62000 out of  63565\n",
      "Making progress at  63000 out of  63565\n"
     ]
    }
   ],
   "source": [
    "# Set the output size\n",
    "output_size = (64, 64)\n",
    "\n",
    "# Define a transform to resize the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(output_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create an empty list to store the resized images as tensors\n",
    "resized_images = []\n",
    "\n",
    "# Loop through each image in the dataset and resize it\n",
    "for i in range( len(dataset['train']) ):\n",
    "    image = dataset['train'][i]['image']  # Get the i-th image and its label\n",
    "    image_resized = transform(image)  # Resize the image\n",
    "    resized_images.append(image_resized)  # Add the resized image to the list\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"Making progress at \",i,\"out of \",len(dataset['train']))\n",
    "\n",
    "# Convert the list of tensors to a tensor stack\n",
    "resized_images_tensor = torch.stack(resized_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63565, 3, 64, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving tensor\n",
    "torch.save(resized_images_tensor, 'images.pt')\n",
    "resized_images_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63565, 3, 64, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading tensor:\n",
    "data = torch.load('./images.pt')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional imports:\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some hyperparam init: #wandb script here\n",
    "bs = 5000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GANs we dont need a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 3, 64, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataloader from our tensor\n",
    "images = DataLoader(data, batch_size=bs, shuffle=False)\n",
    "#batch shape:\n",
    "next(iter(images)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our discriminator is basically a CNN, whereas our generator is basically the equivalent of a cnn but instead of convolutions, we have the gradients of those convolutions with respect to our inputs;\n",
    "\n",
    "Discriminator: Conv2d\n",
    "Generator: ConvTranspose2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 3\n",
    "hs = 64\n",
    "#new dependency:\n",
    "from torch import nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nl, hs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Sequential(\n",
    "                nn.Conv2d(3, hs, kernel_size=3, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(hs),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "        \n",
    "        for i in range(1, nl-1):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                #previous hidden layer size, next hidden layer size from array passed as argument to the constructor\n",
    "                nn.Conv2d(hs*i, hs*i*2, kernel_size=3, stride=1, padding=0), \n",
    "                nn.BatchNorm2d(hs*i*2),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "        \n",
    "        self.layers.append(nn.Sequential(\n",
    "                nn.Conv2d(hs*(nl-1), 1,  kernel_size=3, stride=1, padding=0),\n",
    "                nn.Flatten(),\n",
    "                nn.Sigmoid()\n",
    "            ))\n",
    "    \n",
    "    #forward pass, calls the class init\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            output = layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(192, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis = Discriminator(4,64)\n",
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_size = 256 #same output from discriminator: 256x1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nl, hs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Sequential(\n",
    "            nn.ConvTranspose2d((hs*(2**nl)), (int)((hs*(2**nl))/2), 3, 1, 0), #last hidden size, the one before\n",
    "            nn.BatchNorm2d((hs*(nl-1))/2), \n",
    "            nn.ReLU()\n",
    "        ))\n",
    "        \n",
    "        for i in range(nl-1, 0, -1): #decrementing through the number of layers, this will help with the size in each layer.\n",
    "            self.layer.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d((int)((hs*(2**nl))/(2**i)), (int)((hs*(2**nl))/(2*2**i)),3, 1, 0),\n",
    "                nn.BatchNorm2d((hs*(nl-1))/2), \n",
    "                nn.ReLU()\n",
    "            ))\n",
    "        self.layer.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(hs, 3, 3, 1, 0),\n",
    "                nn.Sigmoid()\n",
    "            ))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.layers(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (float, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of SymInts size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gen \u001b[39m=\u001b[39m Generator(\u001b[39m4\u001b[39;49m, \u001b[39m64\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[83], line 7\u001b[0m, in \u001b[0;36mGenerator.__init__\u001b[0;34m(self, nl, hs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList()\n\u001b[1;32m      5\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mappend(nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m      6\u001b[0m     nn\u001b[39m.\u001b[39mConvTranspose2d((hs\u001b[39m*\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnl)), (\u001b[39mint\u001b[39m)((hs\u001b[39m*\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnl))\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m), \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), \u001b[39m#last hidden size, the one before\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     nn\u001b[39m.\u001b[39;49mBatchNorm2d((hs\u001b[39m*\u001b[39;49m(nl\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m), \n\u001b[1;32m      8\u001b[0m     nn\u001b[39m.\u001b[39mReLU()\n\u001b[1;32m      9\u001b[0m ))\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nl\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m): \u001b[39m#decrementing through the number of layers, this will help with the size in each layer.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer\u001b[39m.\u001b[39mappend(nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     13\u001b[0m         nn\u001b[39m.\u001b[39mConvTranspose2d((\u001b[39mint\u001b[39m)((hs\u001b[39m*\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnl))\u001b[39m/\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mi)), (\u001b[39mint\u001b[39m)((hs\u001b[39m*\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnl))\u001b[39m/\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mi)),\u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m     14\u001b[0m         nn\u001b[39m.\u001b[39mBatchNorm2d((hs\u001b[39m*\u001b[39m(nl\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m), \n\u001b[1;32m     15\u001b[0m         nn\u001b[39m.\u001b[39mReLU()\n\u001b[1;32m     16\u001b[0m     ))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/batchnorm.py:133\u001b[0m, in \u001b[0;36m_BatchNorm.__init__\u001b[0;34m(self, num_features, eps, momentum, affine, track_running_stats, device, dtype)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    123\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    124\u001b[0m     num_features: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     factory_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: device, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m: dtype}\n\u001b[0;32m--> 133\u001b[0m     \u001b[39msuper\u001b[39;49m(_BatchNorm, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    134\u001b[0m         num_features, eps, momentum, affine, track_running_stats, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs\n\u001b[1;32m    135\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/batchnorm.py:48\u001b[0m, in \u001b[0;36m_NormBase.__init__\u001b[0;34m(self, num_features, eps, momentum, affine, track_running_stats, device, dtype)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats \u001b[39m=\u001b[39m track_running_stats\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine:\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty(num_features, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(num_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (float, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of SymInts size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(4, 64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes to self: Re-check prof. alvarez notes for convtranspose2d.\n",
    "\n",
    "Great additional article for convtranspose2d: https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8. Additionally explains why convtranspose2d is not the same as deconvolution.\n",
    "\n",
    "\n",
    "<br> The generator and discriminator should have the same \"in-reverse\" architecture to one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating instances of our two models;\n",
    "dis = Discriminator()\n",
    "gen = Generator()\n",
    "loss = torch.nn.BCELoss() #binary cross entropy: either its classified as real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some hyperparam init: #wandb script here: https://wandb.ai/quickstart, https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=UsM_GTz-742w\n",
    "bs = 5000\n",
    "lr = 0.002\n",
    "#dis_optim = torch.optim.AdamW(dis.parameters(), lr= lr, betas=(0.9,0.99))\n",
    "optimizer = torch.optim.AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 512\n",
      "512 256\n",
      "256 128\n",
      "64 3\n"
     ]
    }
   ],
   "source": [
    "#finding layers for generator:\n",
    "hs=64\n",
    "nl=4\n",
    "print((hs*(2**nl)), (int)((hs*(2**nl))/2))\n",
    "for i in range(1, nl-1):\n",
    "    print((int)((hs*(2**nl))/(2**i)), (int)((hs*(2**nl))/(2*2**i)))\n",
    "\n",
    "print(hs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
